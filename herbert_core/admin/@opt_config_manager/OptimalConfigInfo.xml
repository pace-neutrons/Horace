<?xml version="1.0" encoding="utf-8"?>
<data_struct>
   <win_small>
      <info>Typical configuration for Windows small machine assumes no MPI settings and use OMP if possible</info>
      <hor_config>
         <mem_chunk_size>5000000</mem_chunk_size>
         <ignore_nan>[true]</ignore_nan>
         <ignore_inf>[false]</ignore_inf>
         <use_mex>[true]</use_mex>
         <delete_tmp>[true]</delete_tmp>
      </hor_config>
      <hpc_config>
         <build_sqw_in_parallel>[false]</build_sqw_in_parallel>
         <combine_sqw_using>mex_code</combine_sqw_using>
         <mex_combine_thread_mode>0</mex_combine_thread_mode>
         <mex_combine_buffer_size>65536</mex_combine_buffer_size>
         <parallel_multifit>[false]</parallel_multifit>
      </hpc_config>
      <parallel_config>
         <worker>worker_v4</worker>
         <parallel_workers_number>2</parallel_workers_number>
         <threads>8</threads>
         <parallel_cluster>herbert</parallel_cluster>
         <cluster_config>local</cluster_config>
         <shared_folder_on_local/>
         <shared_folder_on_remote/>
         <working_directory/>
         <external_mpiexec>0</external_mpiexec>
      </parallel_config>
   </win_small>
   <a_mac>
      <info>Typical configuration for MAC machine assumes no MPI settings and no mex code</info>
      <hor_config>
         <mem_chunk_size>5000000</mem_chunk_size>
         <ignore_nan>[true]</ignore_nan>
         <ignore_inf>[false]</ignore_inf>
         <use_mex>[false]</use_mex>
         <delete_tmp>[true]</delete_tmp>
      </hor_config>
      <hpc_config>
         <build_sqw_in_parallel>[false]</build_sqw_in_parallel>
         <combine_sqw_using>matlab</combine_sqw_using>
         <mex_combine_thread_mode>0</mex_combine_thread_mode>
         <mex_combine_buffer_size>65536</mex_combine_buffer_size>
         <parallel_multifit>[false]</parallel_multifit>
      </hpc_config>
      <parallel_config>
         <worker>worker_v4</worker>
         <threads>8</threads>
         <parallel_workers_number>2</parallel_workers_number>
         <parallel_cluster>herbert</parallel_cluster>
         <cluster_config>local</cluster_config>
         <shared_folder_on_local/>
         <shared_folder_on_remote/>
         <working_directory/>
         <external_mpiexec>1</external_mpiexec>
      </parallel_config>
   </a_mac>
   <win_large>
      <info>Typical configuration for Windows large machine assumes use some MPI use OMP</info>
      <hor_config>
         <mem_chunk_size>20000000</mem_chunk_size>
         <ignore_nan>[true]</ignore_nan>
         <ignore_inf>[false]</ignore_inf>
         <use_mex>[true]</use_mex>
         <delete_tmp>[true]</delete_tmp>
      </hor_config>
      <hpc_config>
         <build_sqw_in_parallel>[true]</build_sqw_in_parallel>
         <combine_sqw_using>mex_code</combine_sqw_using>
         <mex_combine_thread_mode>0</mex_combine_thread_mode>
         <mex_combine_buffer_size>131072</mex_combine_buffer_size>
         <parallel_multifit>[false]</parallel_multifit>
      </hpc_config>
      <parallel_config>
         <worker>worker_v4</worker>
         <parallel_cluster>herbert</parallel_cluster>
         <threads>8</threads>
         <parallel_workers_number>4</parallel_workers_number>
         <cluster_config>local</cluster_config>
         <shared_folder_on_local/>
         <shared_folder_on_remote/>
         <working_directory/>
         <external_mpiexec>0</external_mpiexec>
      </parallel_config>
   </win_large>
   <idaaas_small>
      <info>iDaaaS small machine should have mex code but would not benefit from MPI</info>
      <hor_config>
         <mem_chunk_size>5000000</mem_chunk_size>
         <ignore_nan>[true]</ignore_nan>
         <ignore_inf>[false]</ignore_inf>
         <use_mex>[true]</use_mex>
         <delete_tmp>[true]</delete_tmp>
      </hor_config>
      <hpc_config>
         <build_sqw_in_parallel>[false]</build_sqw_in_parallel>
         <combine_sqw_using>mex_code</combine_sqw_using>
         <mex_combine_thread_mode>1</mex_combine_thread_mode>
         <mex_combine_buffer_size>8192</mex_combine_buffer_size>
         <parallel_multifit>[false]</parallel_multifit>
      </hpc_config>
      <parallel_config>
         <worker>worker_v4</worker>
         <parallel_cluster>parpool</parallel_cluster>
         <threads>4</threads>
         <parallel_workers_number>2</parallel_workers_number>
         <cluster_config>default</cluster_config>
         <shared_folder_on_local/>
         <shared_folder_on_remote/>
         <working_directory/>
         <external_mpiexec>0</external_mpiexec>
      </parallel_config>
   </idaaas_small>
   <idaaas_large>
      <info>iDaaaS large machine benefit from mex code and can run some MPI</info>
      <hor_config>
         <mem_chunk_size>20000000</mem_chunk_size>
         <ignore_nan>[true]</ignore_nan>
         <ignore_inf>[false]</ignore_inf>
         <use_mex>[true]</use_mex>
         <delete_tmp>[true]</delete_tmp>
      </hor_config>
      <hpc_config>
         <build_sqw_in_parallel>[true]</build_sqw_in_parallel>
         <combine_sqw_using>mex_code</combine_sqw_using>
         <mex_combine_thread_mode>1</mex_combine_thread_mode>
         <mex_combine_buffer_size>131072</mex_combine_buffer_size>
         <parallel_multifit>[false]</parallel_multifit>
      </hpc_config>
      <parallel_config>
         <worker>worker_v4</worker>
         <parallel_cluster>parpool</parallel_cluster>
         <threads>6</threads>
         <parallel_workers_number>6</parallel_workers_number>
         <cluster_config>default</cluster_config>
         <shared_folder_on_local/>
         <shared_folder_on_remote/>
         <working_directory/>
         <external_mpiexec>0</external_mpiexec>
      </parallel_config>
   </idaaas_large>
   <unix_small>
      <info>General Unix pc would not have mex code so, user should set it up after compiling it himself</info>
      <hor_config>
         <mem_chunk_size>5000000</mem_chunk_size>
         <ignore_nan>[true]</ignore_nan>
         <ignore_inf>[false]</ignore_inf>
         <use_mex>[false]</use_mex>
         <delete_tmp>[true]</delete_tmp>
      </hor_config>
      <hpc_config>
         <build_sqw_in_parallel>[false]</build_sqw_in_parallel>
         <combine_sqw_using>matlab</combine_sqw_using>
         <mex_combine_thread_mode>0</mex_combine_thread_mode>
         <mex_combine_buffer_size>131072</mex_combine_buffer_size>
         <parallel_multifit>[false]</parallel_multifit>
      </hpc_config>
      <parallel_config>
         <worker>worker_v4</worker>
         <parallel_cluster>herbert</parallel_cluster>
         <threads>8</threads>
         <parallel_workers_number>2</parallel_workers_number>
         <cluster_config>local</cluster_config>
         <shared_folder_on_local/>
         <shared_folder_on_remote/>
         <working_directory/>
         <external_mpiexec>0</external_mpiexec>
      </parallel_config>
   </unix_small>
   <unix_large>
      <info>Unix large is our isiscompute machine which uses OMP and MPI and everything is present</info>
      <hor_config>
         <mem_chunk_size>20000000</mem_chunk_size>
         <ignore_nan>[true]</ignore_nan>
         <ignore_inf>[false]</ignore_inf>
         <use_mex>[true]</use_mex>
         <delete_tmp>[true]</delete_tmp>
      </hor_config>
      <hpc_config>
         <build_sqw_in_parallel>[true]</build_sqw_in_parallel>
         <combine_sqw_using>mex_code</combine_sqw_using>
         <mex_combine_thread_mode>1</mex_combine_thread_mode>
         <mex_combine_buffer_size>4096</mex_combine_buffer_size>
         <parallel_multifit>[false]</parallel_multifit>
      </hpc_config>
      <parallel_config>
         <worker>worker_v4</worker>
         <parallel_cluster>parpool</parallel_cluster>
         <threads>8</threads>
         <parallel_workers_number>10</parallel_workers_number>
         <cluster_config>local</cluster_config>
         <shared_folder_on_local/>
         <shared_folder_on_remote/>
         <working_directory/>
         <external_mpiexec>0</external_mpiexec>
      </parallel_config>
   </unix_large>
   <jenkins_win>
      <info>Settings for running on Jenkins Windows build servers</info>
      <hor_config>
         <mem_chunk_size>20000000</mem_chunk_size>
         <ignore_nan>[true]</ignore_nan>
         <ignore_inf>[false]</ignore_inf>
         <use_mex>[true]</use_mex>
         <delete_tmp>[true]</delete_tmp>
      </hor_config>
      <hpc_config>
         <build_sqw_in_parallel>[true]</build_sqw_in_parallel>
         <combine_sqw_using>mex_code</combine_sqw_using>
         <mex_combine_thread_mode>0</mex_combine_thread_mode>
         <mex_combine_buffer_size>131072</mex_combine_buffer_size>
         <parallel_multifit>[false]</parallel_multifit>
      </hpc_config>
      <parallel_config>
         <worker>worker_v4</worker>
         <parallel_cluster>parpool</parallel_cluster>
         <threads>8</threads>
         <parallel_workers_number>4</parallel_workers_number>
         <cluster_config>default</cluster_config>
         <shared_folder_on_local/>
         <shared_folder_on_remote/>
         <working_directory/>
         <external_mpiexec>0</external_mpiexec>
      </parallel_config>
   </jenkins_win>
   <jenkins_unix>
      <info>Settings for running on Jenkins Unix ANVIL build servers</info>
      <hor_config>
         <mem_chunk_size>20000000</mem_chunk_size>
         <ignore_nan>[true]</ignore_nan>
         <ignore_inf>[false]</ignore_inf>
         <use_mex>[true]</use_mex>
         <delete_tmp>[true]</delete_tmp>
      </hor_config>
      <hpc_config>
         <build_sqw_in_parallel>[true]</build_sqw_in_parallel>
         <combine_sqw_using>mex_code</combine_sqw_using>
         <mex_combine_thread_mode>0</mex_combine_thread_mode>
         <mex_combine_buffer_size>131072</mex_combine_buffer_size>
         <parallel_multifit>[false]</parallel_multifit>
      </hpc_config>
      <parallel_config>
         <worker>worker_v4</worker>
         <external_mpiexec>0</external_mpiexec>
         <parallel_cluster>parpool</parallel_cluster>
         <threads>2</threads>
         <parallel_workers_number>2</parallel_workers_number>
         <cluster_config>default</cluster_config>
         <shared_folder_on_local/>
         <shared_folder_on_remote/>
         <working_directory/>
      </parallel_config>
   </jenkins_unix>
</data_struct>
